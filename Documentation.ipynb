{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File organization\n",
    "Python code is stored in pure/py and its subdirectories. pure/data and pure/obj contains folders which store scan data in the form of `.pkl` (serialized numpy arrays) or `.npy` (binary numpy) files, to be used with python. Folder and file names containing the prefix \"1D\" denote a one dimensional scan, \"2D\" denotes a two dimensional scan, and those without any aforementioned prefixes denote angle dependence scans (stationary transducer with rotating metal sample). Images of scan results are placed in subdirectories of pure/scans.\n",
    "\n",
    "## Angle Dependence\n",
    "1. How do you set up the experiment?\n",
    "\n",
    "The setup consists of a metal sample with a micrometer built-in which tilts the sample in a large bucket of water. To increase repeatability, we mark the position of the sample on the bottom surface of the bucket with tape and other pieces of metal.\n",
    "\n",
    "We use a protractor to measure the micrometer reading corresponding to every degree of inclination, starting at 0 degrees and going up to 15 degrees. We perform a linear fit on this data using `scipy.optimize.curve_fit()` and a linear model function $f(\\theta) = a\\theta+b$, where $\\theta$ is the angle of inclination and $f$ is the micrometer reading.\n",
    "\n",
    "2. How do you collect data for the angle dependence experiment?\n",
    "\n",
    "Using the angle to micrometer reading table, we rotate the micrometer to fit a certain angle, place it in the bucket of \n",
    "water, let the water ripples settle down, then save the oscilloscope data. How do we save the oscilloscope data? It is best to use the python console and import `scope` module. Create the output directory, and create a Scope object, passing the directory (as a numpy path) as a parameter. Finally, run `scope.Scope.grab()` to make a single measurement, and the signal will be saved as an `.npy` file in the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py.scope as scope\n",
    "\n",
    "out_folder = \"C:\\\\Users\\\\dionysius\\\\Desktop\\\\PURE\\\\pure\\\\data\\\\TEST_TRANS\"\n",
    "o = scope.Scope(out_folder, filename='test_scope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.grab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How do I analyze a dataset obtained in the angle dependence experiment?\n",
    "\n",
    "You will need to import `py.main`. First, define a variable for the path to the `.npy` data obtained from the scope. Next, instantiate a Transducer object, and call as shown below. Then, call its `write_all()` method and see if the correct peaks have been selected by viewing the figures saved in the subfolder `signal`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How do I do a B-Scan on one angle dependence experiment dataset?\n",
    "First load data into a variable using `load_obj` from `main`. Pass the name (listed in '/obj') of the dataset into the function as a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How do I perform a 1D or 2D scan?\n",
    "\n",
    "Look at `scanning.py`, change the variable `SCAN_FOLDER` at the top to the appropriate name, and in the main program call the class `Scan` with parameters `DIMENSIONS` and `START_POS`. `DIMENSIONS` takes a tuple of real values representing the scanning grid in units of metres. `START_POS` can be 'top right', 'bottom right', etc. Grid formed is an TxMxN numpy matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAFT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import threading\n",
    "from numpy import \\\n",
    "    reshape as np_reshape, \\\n",
    "    power as np_power, \\\n",
    "    sqrt as np_sqrt, \\\n",
    "    argmin as np_argmin, \\\n",
    "    abs as np_abs, \\\n",
    "    sum as np_sum\n",
    "from os import getcwd\n",
    "from os.path import join, dirname\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy` is used for array and general number operations. We alias some `numpy` functions to decrease overhead in the main loop of the program. `threading` is used for multithreading the main program. The functions called from `os` are for system directories and paths to file. `pickle` is used to serialize objects into files. `matplotlib` is used for plotting graphs. `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global min_step, c_0, DEFAULT_ARR_FOLDER\n",
    "global xarr, FD, SD, pbar, T, V, L, T_COMPARE, PRE_OUT, POST_OUT, xni\n",
    "FOLDER_NAME = \"1D-3FOC7in0DEG - 1\"\n",
    "DEFAULT_ARR_FOLDER = join(dirname(getcwd()), \"data\", FOLDER_NAME)\n",
    "FOCAL_DEPTH = 0.0381  # 1.5 inch in metres\n",
    "min_step = 4e-4\n",
    "c_0 = 1498  # water"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we establish folder paths, and define parameters and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arr(output_folder=DEFAULT_ARR_FOLDER):\n",
    "    ftarr = join(output_folder, \"tarr.pkl\")\n",
    "    fvarr = join(output_folder, \"varr.pkl\")\n",
    "    with open(ftarr, 'rb') as rd:\n",
    "        tarr = pickle.load(rd)\n",
    "    with open(fvarr, 'rb') as rd:\n",
    "        varr = pickle.load(rd)\n",
    "    return tarr, varr\n",
    "\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array, dtype=float)\n",
    "    return (np.abs(array - value)).argmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_arr()` outputs time and voltage (2D) arrays. `find_nearest()` finds the index in an array best corresponding to a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tarr, varr = load_arr()\n",
    "tarr = tarr[:, 0, :]\n",
    "varr = varr[:, 0, :]\n",
    "ZERO = find_nearest(tarr[:, 0], 0)\n",
    "T = tarr[ZERO:, 0]  # 1D, time columns all the same\n",
    "V = varr[ZERO:, :]  # 2D\n",
    "FD = find_nearest(T, 2*FOCAL_DEPTH/c_0)  # focal depth\n",
    "# SD = find_nearest(T, 2*SAMPLE_DEPTH/c_0) + 1  # sample depth\n",
    "SD = len(T)-1\n",
    "OFFSET = T[FD]\n",
    "L = np.shape(V)[1]\n",
    "PRE = np.flip(V[:FD, :], axis=0)\n",
    "PRE_T = np.flip(T[:FD], axis=0)\n",
    "PRE_OUT = np.empty(np.shape(PRE))\n",
    "POST = V[FD:SD, :]\n",
    "POST_T = T[FD:SD]\n",
    "POST_OUT = np.empty(np.shape(POST))\n",
    "xarr = np.linspace(-L/2, L/2, L)*min_step\n",
    "xni = np.arange(0, L, 1)\n",
    "tstep = np.mean(T[1:]-T[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(xi):  # xi is imaging index/position\n",
    "    x = xarr[xi]  # x is imaging DISTANCE\n",
    "    ti = 0  # imaging pixel for time/ z direction\n",
    "    while ti < SD:\n",
    "        z = T[ti]*c_0/2\n",
    "        z2 = np_power(z, 2)  # distance, squared\n",
    "        ind = (2/c_0)*np_sqrt(np_power(x-xarr[xni], 2)\n",
    "                              + z2)\n",
    "        zi = (ind/tstep).astype(int)\n",
    "        if ti < FD:  # PRE\n",
    "            PRE_OUT[ti, xi] = np_sum(V[zi[zi < FD], xni[zi < FD]])\n",
    "        if ti >= FD:  # POST\n",
    "            POST_OUT[ti-FD, xi] = np_sum(V[zi[zi < SD], xni[zi < SD]])\n",
    "        ti += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ != '__main__':\n",
    "    # Parallel processing\n",
    "    jobs = []\n",
    "    print(\"Append\")\n",
    "    for i in range(L):\n",
    "        jobs.append(threading.Thread(target=main, args=(i,)))\n",
    "    print(\"Starting\")\n",
    "    for job in jobs:\n",
    "        job.start()\n",
    "    print(\"Joining\")\n",
    "    for job in jobs:\n",
    "        job.join()\n",
    "    print(\"Stitching\")\n",
    "    PRE_OUT = np.flip(PRE_OUT, axis=0)\n",
    "    STITCHED = np.vstack((PRE_OUT, POST_OUT))\n",
    "    pickle.dump(STITCHED, open(join(DEFAULT_ARR_FOLDER,\n",
    "                                    \"SAFT-{}-test.pkl\"\n",
    "                                    .format(FOLDER_NAME)), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10, 10])\n",
    "plt.imshow(STITCHED[:, :], aspect='auto', cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
